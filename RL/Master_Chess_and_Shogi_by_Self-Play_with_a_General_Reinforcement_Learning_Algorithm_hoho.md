### 《Master Chess and Shogi by Self-Play with a General Reinforcement Learning Algorithm》阅读报告

#### 研究现状

本文是Alpha Zero的论文。

当前的棋类智能算法，包括之前的AlphaGo、AlphaGo Zero，打过依赖于其领域知识，难以泛化到其他领域。

Alpha Zero是比AlphaGo Zero更加通用，它用深度神经网络和一个tabula rasa算法，代替传统游戏对战程序中的人工编码知识和特定领域知识。

#### 研究方法

神经网络以当前棋盘的位置s（当前状态）为输入，然后有两个输出：
1. 动作的概率分布 $p_a = Pr(a | s)$
2. 当前状态s下的价值期望  $v = E[z | s]$，z为预期游戏结果（应该就是指立即奖励吧）

即$(p, v) = f_{\theta}(s)$

AlphaZero使用一种通用的MCTS算法，每次搜索都包含一系列的模拟自我对弈过程，表现为对这个蒙特卡洛搜索树从根节点到叶节点的遍历过程。每次模拟对战都从每个状态下，根据当前神经网络，选择一个访问次数低、高移动概率（嗯？）和价值高的节点作为当前的动作，表示为动作概率分布向量$\pi$。

训练过程如下：
1. 随机初始化网络的参数
2. 双方自我对弈，通过MCTS选择动作a，服从分布$\pi_t$
3. 在游戏的最后，对结束状态$s_T$根据游戏规则打分：输则z=-1，平局z=0，赢了z=+1
4. 神经网络输出预测价值v和动作概率分布向量p，最小化v和z的误差，并最大化概率向量p和策略$\pi$的相似度，然后更新神经网络参数。损失函数为：
$l = (z - v)^2 - \pi^Tlogp + c||\theta||^2$
其中c为L2正则化项的超参数。

AlphaZero会持续更新神经网络，而不会等到一个回合结束。自我对弈会使用网络的最新参数，期间不做验证和选择best player。

另外，对于所有类型的游戏，AlphaZero会复用相同的超参数，不会对特别的游戏进行微调。

#### 研究结论

（无）